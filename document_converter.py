# -*- coding: utf-8 -*-

# @Time    : 2025/7/10 15:52
# @Author  : zhangjiya@aviagames.com
# @File    : word_to_markdown.py
# !/usr/bin/env python3
# -*- coding: utf-8 -*-
import re
import requests
from typing import Dict, List, Tuple, Optional
from docx import Document
from docx.oxml.table import CT_Tbl
from docx.oxml.text.paragraph import CT_P
from docx.table import  Table
from docx.text.paragraph import Paragraph
from urllib.parse import urlparse, parse_qs
import time
import os
import json
from datetime import datetime


import sys
from pathlib import Path
sys.path.append(str((Path(__file__).parent / "src").resolve()))
from src.config import config_manager
FEISHU_CONFIG = config_manager.get_config()["feishu"]
# FEISHU_CONFIG = config_manager.get_config().feishu.dict(exclude_unset=False)
print("FEISHU_CONFIG:", FEISHU_CONFIG)
TEMP_DIR = Path("output/tmp")
class DocumentConverter:
    """æ–‡æ¡£è½¬æ¢å™¨ï¼Œæ”¯æŒé£ä¹¦äº‘æ–‡æ¡£å’Œæœ¬åœ°docæ–‡ä»¶è½¬æ¢ä¸ºmarkdown"""

    def __init__(self):
        self.feishu_token = None
        self.token_expire_time = 0
        # ç¡®ä¿ä¸´æ—¶ç›®å½•å­˜åœ¨
        os.makedirs(TEMP_DIR, exist_ok=True)

    def get_feishu_token(self) -> str:
        """è·å–é£ä¹¦è®¿é—®ä»¤ç‰Œ"""
        if self.feishu_token and time.time() < self.token_expire_time:
            return self.feishu_token

        url = f"{FEISHU_CONFIG['base_url']}/auth/v3/tenant_access_token/internal"
        data = {
            "app_id": FEISHU_CONFIG['app_id'],
            "app_secret": FEISHU_CONFIG['app_secret']
        }

        print(f"æ­£åœ¨è·å–é£ä¹¦tokenï¼ŒURL: {url}")
        print(f"è¯·æ±‚æ•°æ®: {data}")

        response = requests.post(url, json=data)
        print(f"é£ä¹¦APIå“åº”çŠ¶æ€ç : {response.status_code}")
        print(f"é£ä¹¦APIå“åº”å†…å®¹: {response.text}")

        if response.status_code == 200:
            result = response.json()
            print(f"è§£æåçš„å“åº”: {result}")

            # æ£€æŸ¥å“åº”æ ¼å¼
            if 'code' in result:
                if result['code'] != 0:
                    raise Exception(f"é£ä¹¦APIè¿”å›é”™è¯¯: {result}")

                if 'tenant_access_token' not in result:
                    raise Exception(f"é£ä¹¦APIå“åº”ä¸­ç¼ºå°‘tenant_access_token: {result}")

                self.feishu_token = result['tenant_access_token']
                self.token_expire_time = time.time() + result.get('expire', 7200) - 60  # æå‰60ç§’åˆ·æ–°
                return self.feishu_token
            else:
                # å¦‚æœæ²¡æœ‰codeå­—æ®µï¼Œå¯èƒ½æ˜¯ç›´æ¥è¿”å›tokençš„æƒ…å†µ
                if 'tenant_access_token' in result:
                    self.feishu_token = result['tenant_access_token']
                    self.token_expire_time = time.time() + result.get('expire', 7200) - 60
                    return self.feishu_token
                else:
                    raise Exception(f"é£ä¹¦APIå“åº”æ ¼å¼å¼‚å¸¸: {result}")
        else:
            raise Exception(f"è·å–é£ä¹¦tokenå¤±è´¥: HTTP {response.status_code}, {response.text}")

    def extract_doc_token_from_url(self, url: str) -> str:
        """ä»é£ä¹¦æ–‡æ¡£URLä¸­æå–æ–‡æ¡£token"""
        parsed = urlparse(url)
        if 'feishu.cn' in parsed.netloc:
            path_parts = parsed.path.split('/')
            # æ”¯æŒå¤šç§é£ä¹¦æ–‡æ¡£ç±»å‹
            if len(path_parts) >= 3:
                if path_parts[1] in ['docx', 'docs']:
                    # é£ä¹¦æ–‡æ¡£: https://xxx.feishu.cn/docx/xxx
                    return path_parts[2]
                elif path_parts[1] == 'wiki':
                    # é£ä¹¦Wiki: https://xxx.feishu.cn/wiki/xxx
                    return path_parts[2]
        raise ValueError("æ— æ•ˆçš„é£ä¹¦æ–‡æ¡£URLï¼Œæ”¯æŒçš„æ ¼å¼ï¼šdocx/docs/wiki")

    def get_feishu_document(self, doc_token: str, url: str = None) -> Dict:
        """è·å–é£ä¹¦æ–‡æ¡£å†…å®¹"""
        token = self.get_feishu_token()
        
        # æ ¹æ®URLç±»å‹é€‰æ‹©ä¸åŒçš„APIç«¯ç‚¹
        if url and 'wiki' in url:
            # Wikiæ–‡æ¡£éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œå…ˆè·å–space_idå’Œnode_token
            return self._get_wiki_document_content(doc_token, token)
        else:
            # æ™®é€šæ–‡æ¡£ä½¿ç”¨docx API
            api_url = f"{FEISHU_CONFIG['base_url']}/docx/v1/documents/{doc_token}/blocks"
            print(f"ä½¿ç”¨Docx API: {api_url}")
            
            headers = {
                "Authorization": f"Bearer {token}",
                "Content-Type": "application/json"
            }

            print(f"æ­£åœ¨è·å–é£ä¹¦æ–‡æ¡£å—ä¿¡æ¯ï¼ŒURL: {api_url}")
            print(f"æ–‡æ¡£token: {doc_token}")

            response = requests.get(api_url, headers=headers)
            print(f"æ–‡æ¡£APIå“åº”çŠ¶æ€ç : {response.status_code}")
            print(f"æ–‡æ¡£APIå“åº”å†…å®¹: {response.text[:500]}...")  # åªæ˜¾ç¤ºå‰500ä¸ªå­—ç¬¦

            if response.status_code == 200:
                result = response.json()

                # æ£€æŸ¥å“åº”æ ¼å¼
                if 'code' in result and result['code'] != 0:
                    raise Exception(f"é£ä¹¦æ–‡æ¡£APIè¿”å›é”™è¯¯: {result}")

                return result
            else:
                raise Exception(f"è·å–é£ä¹¦æ–‡æ¡£å¤±è´¥: HTTP {response.status_code}, {response.text}")

    def _get_wiki_document_content(self, node_token: str, token: str) -> Dict:
        """æŒ‰ç…§å®˜æ–¹æ–‡æ¡£æ­¥éª¤è·å–Wikiæ–‡æ¡£å†…å®¹"""
        print(f"å¼€å§‹è·å–Wikiæ–‡æ¡£å†…å®¹ï¼Œnode_token: {node_token}")
        
        # æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼Œæˆ‘ä»¬éœ€è¦å…ˆè·å–çŸ¥è¯†ç©ºé—´åˆ—è¡¨ï¼Œç„¶åè·å–space_id
        # ä½†æ˜¯è¿™é‡Œæˆ‘ä»¬ç®€åŒ–å¤„ç†ï¼Œç›´æ¥å°è¯•ä½¿ç”¨node_tokenä½œä¸ºdocument_id
        # å› ä¸ºWikiæ–‡æ¡£æœ€ç»ˆä¹Ÿæ˜¯docxæ ¼å¼ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨docx API
        
        try:
            # ç›´æ¥å°è¯•ä½¿ç”¨docx APIè·å–å†…å®¹
            api_url = f"{FEISHU_CONFIG['base_url']}/docx/v1/documents/{node_token}/raw_content"
            headers = {
                "Authorization": f"Bearer {token}",
                "Content-Type": "application/json"
            }
            
            print(f"å°è¯•ç›´æ¥ä½¿ç”¨docx APIè·å–Wikiæ–‡æ¡£å†…å®¹: {api_url}")
            
            response = requests.get(api_url, headers=headers)
            print(f"Wikiæ–‡æ¡£APIå“åº”çŠ¶æ€ç : {response.status_code}")
            # print(f"Wikiæ–‡æ¡£APIå“åº”å†…å®¹: {response.text[:500]}...")
            
            if response.status_code == 200:
                result = response.json()
                
                if 'code' in result and result['code'] != 0:
                    raise Exception(f"è·å–Wikiæ–‡æ¡£å†…å®¹å¤±è´¥: {result}")
                
                return result
            else:
                raise Exception(f"è·å–Wikiæ–‡æ¡£å†…å®¹å¤±è´¥: HTTP {response.status_code}, {response.text}")
                
        except Exception as e:
            print(f"ç›´æ¥ä½¿ç”¨docx APIå¤±è´¥: {e}")
            # å¦‚æœç›´æ¥ä½¿ç”¨docx APIå¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
            raise Exception(f"æ— æ³•è·å–Wikiæ–‡æ¡£å†…å®¹ï¼Œè¯·æ£€æŸ¥æ–‡æ¡£æƒé™å’ŒURLæ ¼å¼: {e}")

    def _extract_space_id_from_node_token(self, node_token: str) -> str:
        """ä»node_tokenä¸­æå–space_id"""
        # è¿™ä¸ªæ–¹æ³•æš‚æ—¶ä¸ä½¿ç”¨ï¼Œå› ä¸ºspace_idéœ€è¦æ˜¯æ•°å­—ç±»å‹
        return "default_space_id"

    def _get_wiki_obj_info(self, node_token: str, token: str) -> Tuple[str, str]:
        """è·å–Wikiæ–‡æ¡£çš„obj_tokenå’Œobj_type"""
        # è¿™ä¸ªæ–¹æ³•æš‚æ—¶ä¸ä½¿ç”¨ï¼Œå› ä¸ºéœ€è¦æ­£ç¡®çš„space_id
        pass

    def _get_wiki_document_text(self, obj_token: str, token: str) -> Dict:
        """è·å–Wikiæ–‡æ¡£çš„çº¯æ–‡æœ¬å†…å®¹"""
        api_url = f"{FEISHU_CONFIG['base_url']}/docx/v1/documents/{obj_token}/raw_content"
        headers = {
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/json"
        }
        
        print(f"è·å–Wikiæ–‡æ¡£æ–‡æœ¬å†…å®¹ï¼ŒURL: {api_url}")
        
        response = requests.get(api_url, headers=headers)
        print(f"Wikiæ–‡æ¡£æ–‡æœ¬APIå“åº”çŠ¶æ€ç : {response.status_code}")
        print(f"Wikiæ–‡æ¡£æ–‡æœ¬APIå“åº”å†…å®¹: {response.text[:500]}...")
        
        if response.status_code == 200:
            result = response.json()
            
            if 'code' in result and result['code'] != 0:
                raise Exception(f"è·å–Wikiæ–‡æ¡£æ–‡æœ¬å¤±è´¥: {result}")
            
            return result
        else:
            raise Exception(f"è·å–Wikiæ–‡æ¡£æ–‡æœ¬å¤±è´¥: HTTP {response.status_code}, {response.text}")

    def get_feishu_document_content(self, doc_token: str, url: str = None) -> Dict:
        """è·å–é£ä¹¦æ–‡æ¡£åŸå§‹å†…å®¹ï¼ˆç”¨äºè·å–æ–‡æœ¬å†…å®¹ï¼‰"""
        token = self.get_feishu_token()
        
        # æ ¹æ®URLç±»å‹é€‰æ‹©ä¸åŒçš„APIç«¯ç‚¹
        if url and 'wiki' in url:
            # Wikiæ–‡æ¡£éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œå¤ç”¨_get_wiki_document_contentæ–¹æ³•
            return self._get_wiki_document_content(doc_token, token)
        else:
            # æ™®é€šæ–‡æ¡£ä½¿ç”¨docx API
            api_url = f"{FEISHU_CONFIG['base_url']}/docx/v1/documents/{doc_token}/raw_content"
            print(f"ä½¿ç”¨Docx APIè·å–å†…å®¹: {api_url}")
            
            headers = {
                "Authorization": f"Bearer {token}",
                "Content-Type": "application/json"
            }

            print(f"æ­£åœ¨è·å–é£ä¹¦æ–‡æ¡£åŸå§‹å†…å®¹ï¼ŒURL: {api_url}")

            response = requests.get(api_url, headers=headers)
            print(f"åŸå§‹å†…å®¹APIå“åº”çŠ¶æ€ç : {response.status_code}")

            if response.status_code == 200:
                result = response.json()

                # æ£€æŸ¥å“åº”æ ¼å¼
                if 'code' in result and result['code'] != 0:
                    raise Exception(f"é£ä¹¦æ–‡æ¡£åŸå§‹å†…å®¹APIè¿”å›é”™è¯¯: {result}")

                return result
            else:
                raise Exception(f"è·å–é£ä¹¦æ–‡æ¡£åŸå§‹å†…å®¹å¤±è´¥: HTTP {response.status_code}, {response.text}")

    def download_feishu_image(self, image_token: str, filename: str) -> str:
        """ä¸‹è½½é£ä¹¦æ–‡æ¡£ä¸­çš„å›¾ç‰‡"""
        token = self.get_feishu_token()
        # ä½¿ç”¨æ­£ç¡®çš„å›¾ç‰‡ä¸‹è½½APIæ¥å£
        url = f"https://open.feishu.cn/open-apis/drive/v1/medias/{image_token}/download"
        headers = {
            "Authorization": f"Bearer {token}"
        }
        # print(f"æ­£åœ¨ä¸‹è½½å›¾ç‰‡ï¼ŒURL: {url}")
        # print(f"å›¾ç‰‡token: {image_token}")
        response = requests.get(url, headers=headers)
        print(f"å›¾ç‰‡ä¸‹è½½å“åº”çŠ¶æ€ç : {response.status_code}")
        if response.status_code == 200:
            image_path = TEMP_DIR / filename
            with open(image_path, 'wb') as f:
                f.write(response.content)
            print(f"å›¾ç‰‡å·²ä¿å­˜åˆ°: {image_path}")
            return str(image_path)
        else:
            raise Exception(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥: HTTP {response.status_code}, {response.text}")

    def convert_feishu_to_markdown(self, url: str, output_path: Optional[str] = None) -> Tuple[str, List[str]]:
        """å°†é£ä¹¦æ–‡æ¡£è½¬æ¢ä¸ºmarkdownï¼ˆç»“åˆåŸå§‹å†…å®¹å’Œå›¾ç‰‡å—ï¼Œä¿è¯å®Œæ•´æ€§ï¼‰"""
        doc_token = self.extract_doc_token_from_url(url)

        # è·å–æ–‡æ¡£å—ä¿¡æ¯ï¼ˆåŒ…å«å›¾ç‰‡ï¼‰
        blocks_data = self.get_feishu_document(doc_token, url)

        # è·å–æ–‡æ¡£åŸå§‹å†…å®¹ï¼ˆç”¨äºè·å–æ–‡æœ¬å’Œæ ‡é¢˜ï¼‰
        content_data = self.get_feishu_document_content(doc_token, url)

        markdown_content = []
        image_counter = [1]  # ç”¨åˆ—è¡¨åŒ…è£¹ï¼Œä¾¿äºé€’å½’æ—¶å¼•ç”¨å’Œè‡ªå¢
        image_paths = []
        table_started = False  # æ ‡è®°è¡¨æ ¼æ˜¯å¦å¼€å§‹

        # å¤„ç†æ–‡æ¡£æ ‡é¢˜
        title = content_data.get('data', {}).get('title', '')
        if not title:
            # å°è¯•ä»å†…å®¹ä¸­æå–æ ‡é¢˜
            content = content_data.get('data', {}).get('content', '')
            if content:
                lines = content.split('\n')
                for line in lines:
                    if line.strip():
                        title = line.strip()
                        break
        if not title:
            title = 'Untitled Document'

        if output_path is None:
            # from config import OUTPUT_DIR
            output_path = str(TEMP_DIR / f"{title}.md")

        markdown_content.append(f"# {title}\n")

        # æ£€æŸ¥æ˜¯å¦ä¸ºWikiæ–‡æ¡£
        if self._is_wiki_document(url):
            # å¤„ç†Wikiæ–‡æ¡£ - ç°åœ¨content_dataå·²ç»æ˜¯çº¯æ–‡æœ¬å†…å®¹
            original_content = content_data.get('data', {}).get('content', '')
            if original_content:
                # æŒ‰è¡Œå¤„ç†åŸå§‹å†…å®¹
                lines = original_content.split('\n')
                for line in lines:
                    line = line.strip()
                    if line:
                        # å¤„ç†æ™®é€šæ–‡æœ¬è¡Œ
                        processed_line = self._process_text_line(line)
                        if processed_line:
                            markdown_content.append(processed_line)
        else:
            # å¤„ç†æ™®é€šæ–‡æ¡£
            # è·å–åŸå§‹å†…å®¹ä½œä¸ºåŸºç¡€æ–‡æœ¬
            original_content = content_data.get('data', {}).get('content', '')
            if original_content:
                # æŒ‰è¡Œå¤„ç†åŸå§‹å†…å®¹
                lines = original_content.split('\n')
                for i, line in enumerate(lines):
                    line = line.strip()
                    if line:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯å›¾ç‰‡æ ‡è®°
                        if line == 'image.png':
                            # æŸ¥æ‰¾å¯¹åº”çš„å›¾ç‰‡å—å¹¶æ’å…¥
                            image_result = self._find_and_process_image(blocks_data, image_counter)
                            if image_result:
                                markdown_content.append(image_result['markdown'] + '\n')
                                image_paths.append(image_result['path'])
                                image_counter[0] += 1
                        else:
                            # å¤„ç†æ™®é€šæ–‡æœ¬è¡Œ
                            processed_line = self._process_text_line(line)
                            if processed_line:
                                markdown_content.append(processed_line)

                                # å¤„ç†è¡¨æ ¼åˆ†éš”ç¬¦
                                if '| **æ—¶é—´** |' in processed_line and not table_started:
                                    table_started = True
                                    # åœ¨è¡¨æ ¼æ ‡é¢˜åæ·»åŠ åˆ†éš”ç¬¦
                                    markdown_content.append("| --- | --- | --- | --- |\n")
                                elif table_started and 'è¯„å®¡çŠ¶æ€' in processed_line:
                                    table_started = False
            else:
                # å¦‚æœæ²¡æœ‰åŸå§‹å†…å®¹ï¼Œä½¿ç”¨blocksç»“æ„
                items = blocks_data.get('data', {}).get('items', [])
                if items:
                    block_map = {block['block_id']: block for block in items}
                    root_block = items[0] if items else None
                    if root_block:
                        self._traverse_block(root_block['block_id'], block_map, markdown_content, image_counter, image_paths)

        md_content = ''.join(markdown_content)

        # å¤„ç†æµç¨‹å›¾
        flowchart_files = []
        if self.detect_flowchart_content(md_content):
            print("ğŸ¨ æ£€æµ‹åˆ°æµç¨‹å›¾ç›¸å…³å†…å®¹ï¼Œæ­£åœ¨ç”Ÿæˆæµç¨‹å›¾...")
            processed_content, flowchart_files = self.process_flowchart_in_content(
                md_content, title, str(Path(output_path).parent)
            )
            md_content = processed_content

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_path_obj = Path(output_path)
        output_path_obj.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path_obj, 'w', encoding='utf-8') as f:
            f.write(md_content)

        # åˆå¹¶å›¾ç‰‡è·¯å¾„å’Œæµç¨‹å›¾è·¯å¾„
        all_files = image_paths + flowchart_files

        return str(output_path_obj), all_files

    def _find_and_process_image(self, blocks_data, image_counter):
        """åœ¨blocksä¸­æŸ¥æ‰¾å¹¶å¤„ç†å›¾ç‰‡"""
        items = blocks_data.get('data', {}).get('items', [])
        image_blocks = []

        # æ”¶é›†æ‰€æœ‰å›¾ç‰‡å—
        for block in items:
            if block.get('block_type') == 27:  # å›¾ç‰‡å—
                image_blocks.append(block)

        # æŒ‰é¡ºåºå¤„ç†å›¾ç‰‡
        if image_counter[0] <= len(image_blocks):
            block = image_blocks[image_counter[0] - 1]
            return self._process_blocks_image(block, image_counter[0])

        return None

    def _process_text_line(self, line):
        """å¤„ç†æ–‡æœ¬è¡Œï¼Œè½¬æ¢ä¸ºmarkdownæ ¼å¼ï¼ˆä¸æœ¬åœ°è½¬æ¢ä¿æŒä¸€è‡´ï¼‰"""
        # å¤„ç†ä¸»æ ‡é¢˜ï¼ˆä¸€çº§æ ‡é¢˜ï¼‰
        if line.startswith('ä¸€ã€') or line.startswith('äºŒã€') or line.startswith('ä¸‰ã€') or \
           line.startswith('å››ã€') or line.startswith('äº”ã€') or line.startswith('å…­ã€') or \
           line.startswith('ä¸ƒã€') or line.startswith('å…«ã€') or line.startswith('ä¹ã€') or \
           line.startswith('åã€') or line.startswith('åä¸€ã€'):
            return f"**{line}**\n\n"

        # å¤„ç†æ–‡æ¡£æ ‡é¢˜å’Œå˜æ›´æ—¥å¿—æ ‡é¢˜
        if line in ['Bç±»æ´»åŠ¨ä¸‰åªå°çŒª', 'æ–‡æ¡£å˜æ›´æ—¥å¿—']:
            return f"**{line}**\n\n"

        # å¤„ç†è¡¨æ ¼æ ‡é¢˜è¡Œ
        if line in ['æ—¶é—´', 'ç‰ˆæœ¬å·', 'å˜æ›´äºº', 'ä¸»è¦å˜æ›´å†…å®¹']:
            return f"| **{line}** |"

        # å¤„ç†è¡¨æ ¼æ•°æ®è¡Œ
        if line in ['2024å¹´5æœˆ29æ—¥', '2024å¹´6æœˆ3æ—¥', '2024å¹´6æœˆ18æ—¥']:
            return f"| {line} |"
        if line in ['1.00', '1.10', '1.20']:
            return f" {line} |"
        if line in ['å­™æ™ºå“²']:
            return f" {line} |"
        if line in ['æ–‡æ¡£å»ºç«‹', 'å†…éƒ¨è®¨è®º', 'è¯„å®¡çŠ¶æ€']:
            return f" {line} |\n"

        # å¤„ç†ä¸‰çº§æ ‡é¢˜ï¼ˆå¸¦ç¼–å·çš„å­æ ‡é¢˜ï¼‰
        if line in ['è§¦å‘é€»è¾‘', 'æ´»åŠ¨å…¥å£', 'ç©æ³•è¯´æ˜']:
            # æ ¹æ®å†…å®¹ç¡®å®šç¼–å·
            if line == 'è§¦å‘é€»è¾‘':
                return f"1. **{line}**\n\n"
            elif line == 'æ´»åŠ¨å…¥å£':
                return f"2. **{line}**\n\n"
            elif line == 'ç©æ³•è¯´æ˜':
                return f"3. **{line}**\n\n"

        # å¤„ç†å››çº§æ ‡é¢˜ï¼ˆå­æ ‡é¢˜ä¸‹çš„æ ‡é¢˜ï¼‰
        if line in ['Slotsè¯´æ˜', 'Respinè¯´æ˜', 'Jackpotè¯´æ˜', 'å…¶ä»–éƒ¨åˆ†è¯´æ˜']:
            return f"**{line}**\n\n"

        # å¤„ç†æ™®é€šæ–‡æœ¬
        return line + "\n\n"

    def _traverse_block(self, block_id, block_map, markdown_content, image_counter, image_paths):
        block = block_map.get(block_id)
        if not block:
            return
        block_type = block.get('block_type', 0)
        # å¤„ç†å½“å‰å—å†…å®¹
        if block_type == 2:  # æ–‡æœ¬æ®µè½
            paragraph_text = self._process_blocks_paragraph(block)
            if paragraph_text:
                markdown_content.append(paragraph_text)
        elif block_type == 31:  # è¡¨æ ¼
            table_text = self._process_blocks_table(block)
            if table_text:
                markdown_content.append(table_text)
        elif block_type == 27:  # å›¾ç‰‡
            image_result = self._process_blocks_image(block, image_counter[0])
            if image_result:
                markdown_content.append(image_result['markdown'] + '\n')
                image_paths.append(image_result['path'])
                image_counter[0] += 1
        elif block_type == 4:  # äºŒçº§æ ‡é¢˜
            heading_text = self._process_blocks_heading(block, 2)
            if heading_text:
                markdown_content.append(heading_text)
        elif block_type == 6:  # å››çº§æ ‡é¢˜
            heading_text = self._process_blocks_heading(block, 4)
            if heading_text:
                markdown_content.append(heading_text)
        elif block_type == 7:  # äº”çº§æ ‡é¢˜
            heading_text = self._process_blocks_heading(block, 5)
            if heading_text:
                markdown_content.append(heading_text)
        elif block_type == 12:  # åˆ—è¡¨é¡¹
            list_text = self._process_blocks_list(block)
            if list_text:
                markdown_content.append(list_text)
        elif block_type == 28:  # ç”»æ¿/ç”»å¸ƒ (Canvas)
            canvas_text = self._process_blocks_canvas(block)
            if canvas_text:
                markdown_content.append(canvas_text)
        elif block_type == 29:  # æµç¨‹å›¾ (Flowchart)
            flowchart_text = self._process_blocks_flowchart(block)
            if flowchart_text:
                markdown_content.append(flowchart_text)
        # é€’å½’å¤„ç†å­å—
        for child_id in block.get('children', []):
            self._traverse_block(child_id, block_map, markdown_content, image_counter, image_paths)

    def convert_docx_to_markdown(self, file_path: str, output_path: Optional[str] = None) -> Tuple[str, List[str]]:
        """å°†æœ¬åœ°docxæ–‡ä»¶è½¬æ¢ä¸ºmarkdown"""
        from shutil import copy2
        doc = Document(file_path)
        markdown_content = []
        # æå–å›¾ç‰‡åˆ°outputç›®å½•
        from config import OUTPUT_DIR
        output_dir = OUTPUT_DIR
        output_dir.mkdir(parents=True, exist_ok=True)
        image_count = 1
        image_paths = []
        for rel in doc.part.rels.values():
            if 'image' in rel.target_ref:
                image_data = rel.target_part.blob
                image_name = f'image_{image_count}.png'
                image_path = output_dir / image_name
                with open(image_path, 'wb') as f:
                    f.write(image_data)
                image_paths.append(str(image_path))
                image_count += 1
        # æ ‡é¢˜
        title = doc.core_properties.title or Path(file_path).stem
        if output_path is None:
            output_path = str(output_dir / f"{title}.md")
        markdown_content.append(f"# {title}\n")
        # æ­£æ–‡
        for element in doc.element.body:
            if isinstance(element, CT_P):
                paragraph = Paragraph(element, doc)
                text = self._process_docx_paragraph(paragraph)
                if text:
                    markdown_content.append(text)
            elif isinstance(element, CT_Tbl):
                table = Table(element, doc)
                table_md = self._process_docx_table(table)
                if table_md:
                    markdown_content.append(table_md)
        # æ–‡æœ«æ’å…¥æ‰€æœ‰å›¾ç‰‡
        for idx, image_path in enumerate(image_paths, 1):
            markdown_content.append(f'![å›¾ç‰‡{idx}](image_{idx}.png)\n')
        md_content = '\n'.join(markdown_content)

        # å¤„ç†æµç¨‹å›¾
        flowchart_files = []
        if self.detect_flowchart_content(md_content):
            print("ğŸ¨ æ£€æµ‹åˆ°æµç¨‹å›¾ç›¸å…³å†…å®¹ï¼Œæ­£åœ¨ç”Ÿæˆæµç¨‹å›¾...")
            processed_content, flowchart_files = self.process_flowchart_in_content(
                md_content, title, str(output_dir)
            )
            md_content = processed_content

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_path_obj = Path(output_path)
        output_path_obj.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path_obj, 'w', encoding='utf-8') as f:
            f.write(md_content)
        
        # åˆå¹¶å›¾ç‰‡è·¯å¾„å’Œæµç¨‹å›¾è·¯å¾„
        all_files = image_paths + flowchart_files
        return str(output_path_obj), all_files

    def _process_paragraph(self, block: Dict) -> str:
        """å¤„ç†é£ä¹¦æ–‡æ¡£æ®µè½"""
        if 'elements' not in block:
            return ""

        text_parts = []
        for element in block['elements']:
            if element['type'] == 'text':
                text = element['text']
                # å¤„ç†æ–‡æœ¬æ ·å¼
                if element.get('style', {}).get('bold'):
                    text = f"**{text}**"
                if element.get('style', {}).get('italic'):
                    text = f"*{text}*"
                if element.get('style', {}).get('underline'):
                    text = f"<u>{text}</u>"
                if element.get('style', {}).get('strikethrough'):
                    text = f"~~{text}~~"
                text_parts.append(text)
            elif element['type'] == 'mention':
                # å¤„ç†@æåŠ
                text_parts.append(f"@{element.get('text', '')}")
            elif element['type'] == 'link':
                # å¤„ç†é“¾æ¥
                link_text = element.get('text', '')
                link_url = element.get('url', '')
                text_parts.append(f"[{link_text}]({link_url})")

        result = ''.join(text_parts)
        if result.strip():  # åªæœ‰å½“æœ‰å†…å®¹æ—¶æ‰è¿”å›
            return result + '\n'
        return ""

    def _process_table(self, block: Dict) -> str:
        """å¤„ç†é£ä¹¦æ–‡æ¡£è¡¨æ ¼"""
        if 'table' not in block:
            return ""

        table = block['table']
        if 'rows' not in table:
            return ""

        markdown_table = []

        for i, row in enumerate(table['rows']):
            row_cells = []
            for cell in row.get('cells', []):
                cell_text = ""
                for element in cell.get('elements', []):
                    if element['type'] == 'text':
                        cell_text += self._process_paragraph_text(element)
                row_cells.append(cell_text)

            markdown_table.append("| " + " | ".join(row_cells) + " |")

            # æ·»åŠ è¡¨å¤´åˆ†éš”ç¬¦
            if i == 0:
                markdown_table.append("| " + " | ".join(["---"] * len(row_cells)) + " |")

        return '\n'.join(markdown_table) + '\n'

    def _process_paragraph_text(self, element: Dict) -> str:
        """å¤„ç†æ®µè½æ–‡æœ¬å…ƒç´ """
        text = element['text']
        if element.get('style', {}).get('bold'):
            text = f"**{text}**"
        if element.get('style', {}).get('italic'):
            text = f"*{text}*"
        if element.get('style', {}).get('underline'):
            text = f"<u>{text}</u>"
        if element.get('style', {}).get('strikethrough'):
            text = f"~~{text}~~"
        return text

    def _process_image(self, block: Dict, counter: int) -> Optional[Dict]:
        """å¤„ç†é£ä¹¦æ–‡æ¡£å›¾ç‰‡"""
        # é€‚é…/blocksæ¥å£çš„å›¾ç‰‡æ•°æ®ç»“æ„
        if 'image' not in block:
            return None

        image = block['image']
        image_token = image.get('token')
        if not image_token:
            # å°è¯•å…¶ä»–å¯èƒ½çš„å­—æ®µå
            image_token = image.get('image_token') or image.get('token_id')

        if not image_token:
            print(f"è­¦å‘Š: å›¾ç‰‡å— {counter} ä¸­æ²¡æœ‰æ‰¾åˆ°token: {image}")
            return None

        try:
            # ä¸‹è½½å›¾ç‰‡
            filename = f"image_{counter}.png"
            image_path = self.download_feishu_image(image_token, filename)

            # ç”Ÿæˆmarkdownå›¾ç‰‡é“¾æ¥
            markdown = f"![å›¾ç‰‡{counter}]({image_path})"

            print(f"æˆåŠŸå¤„ç†å›¾ç‰‡ {counter}: {image_path}")

            return {
                'markdown': markdown,
                'path': image_path
            }
        except Exception as e:
            print(f"å¤„ç†å›¾ç‰‡ {counter} å¤±è´¥: {e}")
            return None

    def _process_heading(self, block: Dict) -> str:
        """å¤„ç†é£ä¹¦æ–‡æ¡£æ ‡é¢˜å—"""
        if 'heading' not in block:
            return ""

        heading = block['heading']
        level = heading.get('level', 2)  # é»˜è®¤äºŒçº§æ ‡é¢˜
        elements = heading.get('elements', [])

        text_parts = []
        for element in elements:
            if element['type'] == 'text':
                text = element['text']
                # å¤„ç†æ–‡æœ¬æ ·å¼
                if element.get('style', {}).get('bold'):
                    text = f"**{text}**"
                if element.get('style', {}).get('italic'):
                    text = f"*{text}*"
                text_parts.append(text)

        heading_text = ''.join(text_parts)
        if heading_text.strip():
            return f"{'#' * level} {heading_text.strip()}\n\n"
        return ""

    def _process_list(self, block: Dict) -> str:
        """å¤„ç†é£ä¹¦æ–‡æ¡£åˆ—è¡¨å—"""
        if 'list' not in block:
            return ""

        list_data = block['list']
        list_type = list_data.get('type', 'unordered')  # ordered æˆ– unordered
        elements = list_data.get('elements', [])

        list_items = []
        for element in elements:
            if element['type'] == 'text':
                text = element['text']
                # å¤„ç†æ–‡æœ¬æ ·å¼
                if element.get('style', {}).get('bold'):
                    text = f"**{text}**"
                if element.get('style', {}).get('italic'):
                    text = f"*{text}*"

                if list_type == 'ordered':
                    list_items.append(f"1. {text}")
                else:
                    list_items.append(f"- {text}")

        if list_items:
            return '\n'.join(list_items) + '\n\n'
        return ""

    def _process_docx_paragraph(self, paragraph: Paragraph) -> str:
        """å¤„ç†docxæ–‡æ¡£æ®µè½"""
        text = paragraph.text.strip()
        if not text:
            return ""

        # å®‰å…¨åœ°è·å–æ®µè½æ ·å¼
        try:
            style = paragraph.style.name.lower() if paragraph.style and paragraph.style.name else ""
        except:
            style = ""

        # æ ¹æ®æ®µè½æ ·å¼æ·»åŠ markdownæ ‡è®°
        if 'heading' in style:
            level = style.replace('heading', '').replace(' ', '')
            if level.isdigit():
                return f"{'#' * int(level)} {text}\n"
            else:
                return f"## {text}\n"
        elif 'list' in style:
            return f"- {text}\n"
        else:
            # å¤„ç†åŠ ç²—ã€æ–œä½“ç­‰æ ·å¼
            run_texts = []
            for run in paragraph.runs:
                run_text = run.text
                if run.bold:
                    run_text = f"**{run_text}**"
                if run.italic:
                    run_text = f"*{run_text}*"
                if run.underline:
                    run_text = f"<u>{run_text}</u>"
                run_texts.append(run_text)
            return ''.join(run_texts) + '\n'

    def _process_docx_table(self, table: Table) -> str:
        """å¤„ç†docxæ–‡æ¡£è¡¨æ ¼"""
        if not table.rows:
            return ""

        markdown_table = []

        for i, row in enumerate(table.rows):
            row_cells = []
            for cell in row.cells:
                cell_text = ""
                for paragraph in cell.paragraphs:
                    cell_text += self._process_docx_paragraph(paragraph).strip()
                row_cells.append(cell_text)

            markdown_table.append("| " + " | ".join(row_cells) + " |")

            # æ·»åŠ è¡¨å¤´åˆ†éš”ç¬¦
            if i == 0:
                markdown_table.append("| " + " | ".join(["---"] * len(row_cells)) + " |")

        return '\n'.join(markdown_table) + '\n'

    def _process_docx_image(self, element, doc, counter: int) -> Optional[Dict]:
        """å¤„ç†docxæ–‡æ¡£ä¸­çš„å›¾ç‰‡"""
        try:
            # è·å–å›¾ç‰‡æ•°æ®
            image_data = None
            for rel in doc.part.rels.values():
                if "image" in rel.target_ref:
                    try:
                        if rel.rId == element.graphic.graphicData.pic.blipFill.blip.embed:
                            image_part = rel.target_part
                            image_data = image_part.blob
                            break
                    except AttributeError:
                        # å°è¯•å…¶ä»–æ–¹å¼è·å–å›¾ç‰‡
                        continue

            if not image_data:
                print(f"æ— æ³•è·å–å›¾ç‰‡ {counter} çš„æ•°æ®")
                return None

            # ä¿å­˜å›¾ç‰‡
            filename = f"image_{counter}.png"
            image_path = TEMP_DIR / filename
            with open(image_path, 'wb') as f:
                f.write(image_data)

            print(f"å·²ä¿å­˜å›¾ç‰‡: {image_path}")

            # ç”Ÿæˆmarkdownå›¾ç‰‡é“¾æ¥
            markdown = f"![å›¾ç‰‡{counter}]({image_path})"

            return {
                'markdown': markdown,
                'path': str(image_path)
            }
        except Exception as e:
            print(f"å¤„ç†å›¾ç‰‡å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _process_blocks_image(self, block: Dict, counter: int) -> Optional[Dict]:
        """å¤„ç†/blocksæ¥å£è¿”å›çš„å›¾ç‰‡å—"""
        if 'image' not in block:
            return None

        image = block['image']
        image_token = image.get('token')
        if not image_token:
            print(f"è­¦å‘Š: å›¾ç‰‡å— {counter} ä¸­æ²¡æœ‰æ‰¾åˆ°token: {image}")
            return None

        try:
            # ä¸‹è½½å›¾ç‰‡
            filename = f"image_{counter}.png"
            image_path = self.download_feishu_image(image_token, filename)

            # ç”Ÿæˆmarkdownå›¾ç‰‡é“¾æ¥
            markdown = f"![å›¾ç‰‡{counter}]({image_path})"

            print(f"æˆåŠŸå¤„ç†å›¾ç‰‡ {counter}: {image_path}")

            return {
                'markdown': markdown,
                'path': image_path
            }
        except Exception as e:
            print(f"å¤„ç†å›¾ç‰‡ {counter} å¤±è´¥: {e}")
            return None

    def _process_blocks_paragraph(self, block: Dict) -> str:
        """å¤„ç†/blocksæ¥å£è¿”å›çš„æ®µè½å—"""
        if 'text' not in block:
            return ""

        text_data = block['text']
        elements = text_data.get('elements', [])

        text_parts = []
        for element in elements:
            if 'text_run' in element:
                text_run = element['text_run']
                content = text_run.get('content', '')
                style = text_run.get('text_element_style', {})

                # å¤„ç†æ–‡æœ¬æ ·å¼
                if style.get('bold'):
                    content = f"**{content}**"
                if style.get('italic'):
                    content = f"*{content}*"
                if style.get('underline'):
                    content = f"<u>{content}</u>"
                if style.get('strikethrough'):
                    content = f"~~{content}~~"

                text_parts.append(content)

        result = ''.join(text_parts)
        if result.strip():  # åªæœ‰å½“æœ‰å†…å®¹æ—¶æ‰è¿”å›
            return result + '\n'
        return ""

    def _process_blocks_heading(self, block: Dict, level: int) -> str:
        """å¤„ç†/blocksæ¥å£è¿”å›çš„æ ‡é¢˜å—"""
        # æ ¹æ®block_typeç¡®å®šæ ‡é¢˜å­—æ®µå
        heading_fields = {
            4: 'heading2',
            6: 'heading4',
            7: 'heading5'
        }

        field_name = heading_fields.get(level, 'heading2')
        if field_name not in block:
            return ""

        heading_data = block[field_name]
        elements = heading_data.get('elements', [])

        text_parts = []
        for element in elements:
            if 'text_run' in element:
                text_run = element['text_run']
                content = text_run.get('content', '')
                style = text_run.get('text_element_style', {})

                # å¤„ç†æ–‡æœ¬æ ·å¼
                if style.get('bold'):
                    content = f"**{content}**"
                if style.get('italic'):
                    content = f"*{content}*"

                text_parts.append(content)

        heading_text = ''.join(text_parts)
        if heading_text.strip():
            return f"{'#' * level} {heading_text.strip()}\n\n"
        return ""

    def _process_blocks_list(self, block: Dict) -> str:
        """å¤„ç†/blocksæ¥å£è¿”å›çš„åˆ—è¡¨å—"""
        if 'text' not in block:
            return ""

        text_data = block['text']
        elements = text_data.get('elements', [])

        text_parts = []
        for element in elements:
            if 'text_run' in element:
                text_run = element['text_run']
                content = text_run.get('content', '')
                style = text_run.get('text_element_style', {})

                # å¤„ç†æ–‡æœ¬æ ·å¼
                if style.get('bold'):
                    content = f"**{content}**"
                if style.get('italic'):
                    content = f"*{content}*"

                text_parts.append(content)

        list_text = ''.join(text_parts)
        if list_text.strip():
            return f"- {list_text.strip()}\n"
        return ""

    def _process_blocks_table(self, block: Dict) -> str:
        """å¤„ç†/blocksæ¥å£è¿”å›çš„è¡¨æ ¼å—"""
        # è¡¨æ ¼å¤„ç†æ¯”è¾ƒå¤æ‚ï¼Œéœ€è¦è·å–å­å—ä¿¡æ¯
        # è¿™é‡Œå…ˆè¿”å›ä¸€ä¸ªç®€å•çš„è¡¨æ ¼æ ‡è®°
        return "\n[è¡¨æ ¼å†…å®¹]\n\n"

    def _process_blocks_canvas(self, block: Dict) -> str:
        """å¤„ç†/blocksæ¥å£è¿”å›çš„ç”»æ¿å—"""
        if 'canvas' not in block:
            return ""
        
        canvas_data = block['canvas']
        canvas_id = canvas_data.get('id', '')
        canvas_title = canvas_data.get('title', 'ç”»æ¿')
        
        # å°è¯•è·å–ç”»æ¿å†…å®¹
        canvas_content = canvas_data.get('content', '')
        canvas_elements = canvas_data.get('elements', [])
        
        # ç”Ÿæˆç”»æ¿æ ‡è®°
        canvas_markdown = f"\n\n## {canvas_title}\n\n"
        
        if canvas_content:
            # å¦‚æœæœ‰æ–‡æœ¬å†…å®¹ï¼Œç›´æ¥ä½¿ç”¨
            canvas_markdown += f"{canvas_content}\n\n"
        elif canvas_elements:
            # å¦‚æœæœ‰å…ƒç´ ï¼Œå°è¯•è§£æ
            canvas_markdown += f"```canvas\n"
            for element in canvas_elements:
                element_type = element.get('type', 'unknown')
                element_content = element.get('content', '')
                if element_content:
                    canvas_markdown += f"{element_type}: {element_content}\n"
            canvas_markdown += f"```\n\n"
        else:
            # é»˜è®¤ç”»æ¿æ ‡è®°
            canvas_markdown += f"```canvas\n"
            canvas_markdown += f"ç”»æ¿ID: {canvas_id}\n"
            canvas_markdown += f"```\n\n"
        
        print(f"âœ… æ£€æµ‹åˆ°ç”»æ¿å†…å®¹: {canvas_title}")
        return canvas_markdown

    def _process_blocks_flowchart(self, block: Dict) -> str:
        """å¤„ç†/blocksæ¥å£è¿”å›çš„æµç¨‹å›¾å—"""
        if 'flowchart' not in block:
            return ""
        
        flowchart_data = block['flowchart']
        flowchart_id = flowchart_data.get('id', '')
        flowchart_title = flowchart_data.get('title', 'æµç¨‹å›¾')
        
        # å°è¯•è·å–æµç¨‹å›¾å†…å®¹
        flowchart_content = flowchart_data.get('content', '')
        flowchart_elements = flowchart_data.get('elements', [])
        
        # ç”Ÿæˆæµç¨‹å›¾æ ‡è®°
        flowchart_markdown = f"\n\n## {flowchart_title}\n\n"
        
        if flowchart_content:
            # å¦‚æœæœ‰Mermaidå†…å®¹ï¼Œç›´æ¥ä½¿ç”¨
            if 'mermaid' in flowchart_content.lower() or 'flowchart' in flowchart_content.lower():
                flowchart_markdown += f"```mermaid\n{flowchart_content}\n```\n\n"
            else:
                # å°è¯•è½¬æ¢ä¸ºMermaidæ ¼å¼
                mermaid_content = self._convert_to_mermaid(flowchart_content)
                flowchart_markdown += f"```mermaid\n{mermaid_content}\n```\n\n"
        elif flowchart_elements:
            # å¦‚æœæœ‰å…ƒç´ ï¼Œå°è¯•è§£æä¸ºMermaid
            mermaid_content = self._convert_elements_to_mermaid(flowchart_elements)
            flowchart_markdown += f"```mermaid\n{mermaid_content}\n```\n\n"
        else:
            # é»˜è®¤æµç¨‹å›¾æ ‡è®°
            flowchart_markdown += f"```mermaid\n"
            flowchart_markdown += f"flowchart TD\n"
            flowchart_markdown += f"    A[\"å¼€å§‹\"]\n"
            flowchart_markdown += f"    B[\"æµç¨‹å›¾å†…å®¹\"]\n"
            flowchart_markdown += f"    C[\"ç»“æŸ\"]\n"
            flowchart_markdown += f"    A --> B\n"
            flowchart_markdown += f"    B --> C\n"
            flowchart_markdown += f"```\n\n"
        
        print(f"âœ… æ£€æµ‹åˆ°æµç¨‹å›¾å†…å®¹: {flowchart_title}")
        return flowchart_markdown

    def _convert_to_mermaid(self, content: str) -> str:
        """å°†æ–‡æœ¬å†…å®¹è½¬æ¢ä¸ºMermaidæ ¼å¼"""
        lines = content.split('\n')
        mermaid_lines = ["flowchart TD"]
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # ç®€å•çš„æ–‡æœ¬åˆ°Mermaidè½¬æ¢
            if any(keyword in line for keyword in ['å¼€å§‹', 'start', 'å¯åŠ¨']):
                mermaid_lines.append(f"    A[\"{line}\"]")
            elif any(keyword in line for keyword in ['ç»“æŸ', 'end', 'å®Œæˆ']):
                mermaid_lines.append(f"    B[\"{line}\"]")
            elif any(keyword in line for keyword in ['åˆ¤æ–­', 'decision', 'æ˜¯å¦']):
                mermaid_lines.append(f"    C{{\"{line}\"}}")
            else:
                mermaid_lines.append(f"    D[\"{line}\"]")
        
        # æ·»åŠ è¿æ¥
        for i in range(len(mermaid_lines) - 1):
            if i > 0:  # è·³è¿‡ç¬¬ä¸€è¡Œï¼ˆflowchart TDï¼‰
                current_id = chr(ord('A') + i - 1)
                next_id = chr(ord('A') + i)
                mermaid_lines.append(f"    {current_id} --> {next_id}")
        
        return '\n'.join(mermaid_lines)

    def _convert_elements_to_mermaid(self, elements: List[Dict]) -> str:
        """å°†æµç¨‹å›¾å…ƒç´ è½¬æ¢ä¸ºMermaidæ ¼å¼"""
        mermaid_lines = ["flowchart TD"]
        
        for i, element in enumerate(elements):
            element_type = element.get('type', 'unknown')
            element_content = element.get('content', f'æ­¥éª¤{i+1}')
            
            if element_type == 'start':
                mermaid_lines.append(f"    A[\"{element_content}\"]")
            elif element_type == 'end':
                mermaid_lines.append(f"    B[\"{element_content}\"]")
            elif element_type == 'decision':
                mermaid_lines.append(f"    C{{\"{element_content}\"}}")
            else:
                mermaid_lines.append(f"    D{i}[\"{element_content}\"]")
        
        return '\n'.join(mermaid_lines)

    def convert_document(self, source: str, output_path: Optional[str] = None) -> Tuple[str, List[str]]:
        """ä¸»è½¬æ¢æ–¹æ³•ï¼Œæ ¹æ®è¾“å…¥ç±»å‹è‡ªåŠ¨é€‰æ‹©è½¬æ¢æ–¹å¼"""
        if source.startswith('http'):
            # é£ä¹¦äº‘æ–‡æ¡£
            return self.convert_feishu_to_markdown(source, output_path)
        else:
            # æœ¬åœ°æ–‡ä»¶
            file_path = Path(source)
            if not file_path.exists():
                raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {source}")

            if file_path.suffix.lower() == '.docx':
                return self.convert_docx_to_markdown(str(file_path), output_path)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")

    def normalize_markdown_content(self, content: str) -> str:
        """æ ‡å‡†åŒ–markdownå†…å®¹ï¼Œç¡®ä¿æ ¼å¼ä¸€è‡´"""
        lines = content.split('\n')
        normalized_lines = []

        for line in lines:
            # ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
            if not line.strip() and normalized_lines and not normalized_lines[-1].strip():
                continue

            # æ ‡å‡†åŒ–æ ‡é¢˜æ ¼å¼
            if line.startswith('#'):
                # ç¡®ä¿æ ‡é¢˜å‰åæœ‰ç©ºè¡Œ
                if normalized_lines and normalized_lines[-1].strip():
                    normalized_lines.append('')
                normalized_lines.append(line)
                normalized_lines.append('')
            else:
                normalized_lines.append(line)

        # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„ç©ºè¡Œ
        while normalized_lines and not normalized_lines[0].strip():
            normalized_lines.pop(0)
        while normalized_lines and not normalized_lines[-1].strip():
            normalized_lines.pop()

        return '\n'.join(normalized_lines)

    def convert_document_unified(self, source: str, output_path: Optional[str] = None) -> Tuple[str, List[str]]:
        """ç»Ÿä¸€çš„æ–‡æ¡£è½¬æ¢æ–¹æ³•ï¼Œç¡®ä¿ç»“æœæ ¼å¼ä¸€è‡´"""
        # æ‰§è¡Œè½¬æ¢
        result_path, image_paths = self.convert_document(source, output_path)
        # è¯»å–è½¬æ¢ç»“æœ
        with open(result_path, 'r', encoding='utf-8') as f:
            content = f.read()
        # æ ‡å‡†åŒ–å†…å®¹
        normalized_content = self.normalize_markdown_content(content)
        # é‡æ–°ä¿å­˜
        with open(result_path, 'w', encoding='utf-8') as f:
            f.write(normalized_content)
        # print("[æ–‡æ¡£è§£æå†…å®¹å‰1000å­—ç¬¦]:\n", normalized_content[:1000])
        return result_path, image_paths

    def _process_wiki_content(self, wiki_data: Dict) -> str:
        """å¤„ç†Wikiæ–‡æ¡£çš„æ•°æ®ç»“æ„"""
        if 'data' not in wiki_data:
            return ""
        
        wiki_content = []
        data = wiki_data['data']
        
        # å¤„ç†Wikiæ–‡æ¡£çš„æ ‡é¢˜
        title = data.get('title', 'Untitled Wiki')
        wiki_content.append(f"# {title}\n")
        
        # å¤„ç†Wikiæ–‡æ¡£çš„èŠ‚ç‚¹å†…å®¹
        nodes = data.get('nodes', [])
        for node in nodes:
            node_type = node.get('type', '')
            node_content = node.get('content', {})
            
            if node_type == 'heading':
                # å¤„ç†æ ‡é¢˜
                level = node_content.get('level', 2)
                text = node_content.get('text', '')
                wiki_content.append(f"{'#' * level} {text}\n\n")
            
            elif node_type == 'paragraph':
                # å¤„ç†æ®µè½
                text = node_content.get('text', '')
                if text.strip():
                    wiki_content.append(f"{text}\n\n")
            
            elif node_type == 'list':
                # å¤„ç†åˆ—è¡¨
                items = node_content.get('items', [])
                for item in items:
                    list_type = item.get('type', 'unordered')
                    text = item.get('text', '')
                    if list_type == 'ordered':
                        wiki_content.append(f"1. {text}\n")
                    else:
                        wiki_content.append(f"- {text}\n")
                wiki_content.append("\n")
            
            elif node_type == 'table':
                # å¤„ç†è¡¨æ ¼
                rows = node_content.get('rows', [])
                for i, row in enumerate(rows):
                    cells = row.get('cells', [])
                    row_text = "| " + " | ".join(cells) + " |"
                    wiki_content.append(row_text + "\n")
                    
                    # æ·»åŠ è¡¨å¤´åˆ†éš”ç¬¦
                    if i == 0:
                        separator = "| " + " | ".join(["---"] * len(cells)) + " |"
                        wiki_content.append(separator + "\n")
                wiki_content.append("\n")
            
            elif node_type == 'image':
                # å¤„ç†å›¾ç‰‡
                image_url = node_content.get('url', '')
                alt_text = node_content.get('alt', 'å›¾ç‰‡')
                wiki_content.append(f"![{alt_text}]({image_url})\n\n")
        
        return ''.join(wiki_content)

    def _is_wiki_document(self, url: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºWikiæ–‡æ¡£"""
        return 'wiki' in url

    def detect_flowchart_content(self, content: str) -> bool:
        """æ£€æµ‹å†…å®¹ä¸­æ˜¯å¦åŒ…å«æµç¨‹å›¾ç›¸å…³å…³é”®è¯"""
        # æ›´çµæ´»çš„æµç¨‹å›¾æ£€æµ‹å…³é”®è¯
        flowchart_keywords = [
            'æµç¨‹å›¾', 'æµç¨‹', 'diagram', 'ç”»æ¿', 'canvas', 'mermaid',
            'flowchart', 'æ—¶åºå›¾', 'æ¶æ„å›¾', 'æ€ç»´å¯¼å›¾', 'mindmap',
            'å¼€å§‹', 'ç»“æŸ', 'åˆ¤æ–­', 'å¤„ç†', 'å†³ç­–', 'åˆ†æ”¯', 'å¾ªç¯',
            'start', 'end', 'decision', 'process', 'action'
        ]
        return any(keyword.lower() in content.lower() for keyword in flowchart_keywords)

    def create_flowchart_mermaid(self, content: str, title: str = "æµç¨‹å›¾") -> str:
        """åŸºäºå†…å®¹åˆ›å»ºMermaidæµç¨‹å›¾"""
        mermaid_content = f"""---
title: {title}
---
flowchart TD
"""
        
        # åŸºäºå¤ºå®å¥‡å…µæ´»åŠ¨çš„ç‰¹å®šæµç¨‹
        if "æ´»åŠ¨å¼€å¯" in content:
            mermaid_content += """    A1["æ´»åŠ¨å¼€å¯"]
    P1["æ”¯æŒæ ‡ç­¾è§¦å‘ Playå¸§å¼¹å‡ºå¼€å‘ŠçŸ¥"]
    P2["ç‚¹å‡»è·³è½¬è‡³æ´»åŠ¨ä¸»UI"]
    D1{"æ˜¯å¦é¦–æ¬¡è¿›å…¥æ´»åŠ¨"}
    P3["æ–°æ‰‹å¼•å¯¼åŠèµ é€ é…ç½®çš„é“å…·æ•°é‡"]
    P4["æ´»åŠ¨è½´ä½Cashåœºè·å–"]
    P5["éæ¥ç”¨æˆ·å¯ç”¨å¹¿å‘Šå…‘æ¢"]
    D2{"é“å…·æ•°é‡"}
    P6["ä»ä¸‰é€‰ä¸€ç›¸ä½è¿›å…¥ æ¸¸æˆä¸»UI"]
    P7["æ´»åŠ¨ä¸“å±é“å…·ç¤¼åŒ…"]
    P8["ç”¨æˆ·é“å…·ä¸å¤Ÿæ¸¸æˆ æ—¶éœ€ä¸»åŠ¨å¼¹å‡º"]
    E1["è‡ªé€‰å½“å‰å…³å¡æ ¼å­"]

    %% è¿æ¥å…³ç³»
    A1 --> P1
    P1 --> P2
    P2 --> D1
    D1 -->|æ˜¯| P3
    D1 -->|å¦| P4
    P4 --> P5
    P3 --> D2
    P5 --> D2
    D2 -->|å……è¶³| P6
    D2 -->|ä¸è¶³| P7
    P6 --> E1
    P7 --> P8
    P8 --> P7
"""
        else:
            # é€šç”¨æµç¨‹å›¾ç”Ÿæˆ
            lines = content.split('\n')
            nodes = []
            connections = []
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                # è¯†åˆ«èŠ‚ç‚¹
                if any(keyword in line for keyword in ['æ´»åŠ¨å¼€å¯', 'å¼€å§‹', 'å¯åŠ¨']):
                    nodes.append(('A', line))
                elif any(keyword in line for keyword in ['æ˜¯å¦', 'åˆ¤æ–­', 'æ£€æŸ¥', 'éªŒè¯']):
                    nodes.append(('D', line))
                elif any(keyword in line for keyword in ['ç»“æŸ', 'å®Œæˆ', 'é€€å‡º', 'å…³é—­']):
                    nodes.append(('E', line))
                else:
                    nodes.append(('P', line))
            
            # ç”ŸæˆèŠ‚ç‚¹
            for i, (node_type, node_text) in enumerate(nodes):
                node_id = f"{node_type}{i+1}"
                if node_type == 'D':
                    mermaid_content += f"    {node_id}{{\"{node_text}\"}}\n"
                else:
                    mermaid_content += f"    {node_id}[\"{node_text}\"]\n"
            
            # ç”Ÿæˆç®€å•è¿æ¥
            for i in range(len(nodes) - 1):
                current_id = f"{nodes[i][0]}{i+1}"
                next_id = f"{nodes[i+1][0]}{i+2}"
                mermaid_content += f"    {current_id} --> {next_id}\n"
        
        return mermaid_content


    def process_flowchart_in_content(self, content: str, title: str, output_dir: str) -> Tuple[str, List[str]]:
        """å¤„ç†å†…å®¹ä¸­çš„æµç¨‹å›¾"""
        flowchart_files = []
        
        if self.detect_flowchart_content(content):
            print("ğŸ¨ æ£€æµ‹åˆ°æµç¨‹å›¾ç›¸å…³å†…å®¹ï¼Œæ­£åœ¨ç”Ÿæˆæµç¨‹å›¾...")
            
            # ç”ŸæˆMermaidæµç¨‹å›¾å†…å®¹
            mermaid_content = self.create_flowchart_mermaid(content, title)
            
            # æ™ºèƒ½æŸ¥æ‰¾æµç¨‹å›¾æ’å…¥ä½ç½®
            flowchart_text = f"\n\n## æµç¨‹å›¾\n\n```mermaid\n{mermaid_content}\n```\n\n"
            
            # å°è¯•å¤šç§æ–¹å¼æŸ¥æ‰¾åˆé€‚çš„æ’å…¥ä½ç½®
            insertion_position = self._find_flowchart_insertion_position(content)
            
            if insertion_position > 0:
                # åœ¨æ‰¾åˆ°çš„ä½ç½®æ’å…¥æµç¨‹å›¾
                content = content[:insertion_position] + flowchart_text + content[insertion_position:]
                print(f"âœ… æµç¨‹å›¾å·²æ’å…¥åˆ°ä½ç½® {insertion_position}")
            else:
                # å¦‚æœæ‰¾ä¸åˆ°åˆé€‚ä½ç½®ï¼Œæ·»åŠ åˆ°æ–‡æ¡£æœ«å°¾
                content += flowchart_text
                print("âœ… æµç¨‹å›¾å·²æ·»åŠ åˆ°æ–‡æ¡£æœ«å°¾")
            
            print("âœ… æµç¨‹å›¾å·²æˆåŠŸåµŒå…¥åˆ°Markdownå†…å®¹ä¸­")
        
        return content, flowchart_files

    def _find_flowchart_insertion_position(self, content: str) -> int:
        """æ™ºèƒ½æŸ¥æ‰¾æµç¨‹å›¾æ’å…¥ä½ç½® - åŠ¨æ€åˆ†ææ–‡æ¡£ç»“æ„"""
        lines = content.split('\n')
        
        # 1. åˆ†ææ–‡æ¡£ç»“æ„ï¼Œæ‰¾åˆ°æ‰€æœ‰æ ‡é¢˜
        headings = self._analyze_document_structure(lines)
        
        # 2. æ ¹æ®æµç¨‹å›¾å†…å®¹ç‰¹å¾ï¼Œæ‰¾åˆ°æœ€åˆé€‚çš„æ’å…¥ä½ç½®
        best_position = self._find_best_insertion_position(content, lines, headings)
        
        return best_position

    def _analyze_document_structure(self, lines: List[str]) -> List[Dict]:
        """åˆ†ææ–‡æ¡£ç»“æ„ï¼Œæå–æ‰€æœ‰æ ‡é¢˜ä¿¡æ¯"""
        headings = []
        
        for i, line in enumerate(lines):
            line = line.strip()
            if not line:
                continue
                
            # æ£€æµ‹æ ‡é¢˜æ ¼å¼
            heading_info = self._detect_heading_format(line, i)
            if heading_info:
                headings.append(heading_info)
        
        return headings

    def _detect_heading_format(self, line: str, line_num: int) -> Optional[Dict]:
        """æ£€æµ‹æ ‡é¢˜æ ¼å¼"""
        # Markdownæ ‡é¢˜ (# ## ###)
        if line.startswith('#'):
            level = len(line) - len(line.lstrip('#'))
            if level <= 6:  # åªå¤„ç†1-6çº§æ ‡é¢˜
                text = line.lstrip('#').strip()
                return {
                    'type': 'markdown',
                    'level': level,
                    'text': text,
                    'line_num': line_num,
                    'original_line': line
                }
        
        # åŠ ç²—æ ‡é¢˜ (**text**)
        if line.startswith('**') and line.endswith('**') and len(line) > 4:
            text = line[2:-2].strip()
            # æ ¹æ®å†…å®¹åˆ¤æ–­çº§åˆ«
            level = self._guess_heading_level(text)
            return {
                'type': 'bold',
                'level': level,
                'text': text,
                'line_num': line_num,
                'original_line': line
            }
        
        # å…¶ä»–å¯èƒ½çš„æ ‡é¢˜æ ¼å¼
        if self._is_likely_heading(line):
            level = self._guess_heading_level(line)
            return {
                'type': 'other',
                'level': level,
                'text': line,
                'line_num': line_num,
                'original_line': line
            }
        
        return None

    def _guess_heading_level(self, text: str) -> int:
        """æ ¹æ®æ ‡é¢˜å†…å®¹çŒœæµ‹çº§åˆ«"""
        # ä¸€çº§æ ‡é¢˜ç‰¹å¾
        if any(keyword in text for keyword in ['æ¦‚è¿°', 'ä»‹ç»', 'æ€»è§ˆ', 'overview', 'introduction']):
            return 1
        
        # äºŒçº§æ ‡é¢˜ç‰¹å¾
        if any(keyword in text for keyword in ['æµç¨‹', 'åŠŸèƒ½', 'æ´»åŠ¨', 'è®¾è®¡', 'å®ç°', 'é…ç½®', 'è¯´æ˜']):
            return 2
        
        # ä¸‰çº§æ ‡é¢˜ç‰¹å¾
        if any(keyword in text for keyword in ['è¯¦ç»†', 'å…·ä½“', 'æ­¥éª¤', 'æ–¹æ³•', 'æ“ä½œ']):
            return 3
        
        # é»˜è®¤äºŒçº§æ ‡é¢˜
        return 2

    def _is_likely_heading(self, line: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦å¯èƒ½æ˜¯æ ‡é¢˜"""
        # çŸ­è¡Œä¸”åŒ…å«å…³é”®è¯
        if len(line) < 50 and any(keyword in line for keyword in [
            'æµç¨‹', 'åŠŸèƒ½', 'æ´»åŠ¨', 'è®¾è®¡', 'å®ç°', 'é…ç½®', 'è¯´æ˜', 'è¯¦ç»†', 'æ­¥éª¤', 'æ–¹æ³•'
        ]):
            return True
        
        # åŒ…å«ç¼–å·çš„æ ‡é¢˜
        if re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ã€.]', line):
            return True
        
        return False

    def _find_best_insertion_position(self, content: str, lines: List[str], headings: List[Dict]) -> int:
        """æ ¹æ®æ–‡æ¡£ç»“æ„æ‰¾åˆ°æœ€ä½³æ’å…¥ä½ç½®"""
        if not headings:
            return -1
        
        # ç­–ç•¥1: æŸ¥æ‰¾åŒ…å«æµç¨‹å›¾ç›¸å…³å…³é”®è¯çš„æ ‡é¢˜ï¼ˆæ’é™¤å·²æ’å…¥çš„æµç¨‹å›¾æ ‡é¢˜ï¼‰
        # æŒ‰ä¼˜å…ˆçº§æ’åºå…³é”®è¯ï¼Œæ›´å…·ä½“çš„ä¼˜å…ˆ
        flowchart_keywords_priority = [
            ['æµç¨‹'],  # æœ€é«˜ä¼˜å…ˆçº§
            ['åŠŸèƒ½', 'è®¾è®¡', 'å®ç°'],  # é«˜ä¼˜å…ˆçº§
            ['æ´»åŠ¨', 'æ“ä½œ', 'æ­¥éª¤']  # ä½ä¼˜å…ˆçº§
        ]
        
        # æŒ‰ä¼˜å…ˆçº§æŸ¥æ‰¾
        for priority_keywords in flowchart_keywords_priority:
            for heading in headings:
                # æ’é™¤å·²ç»æ’å…¥çš„æµç¨‹å›¾æ ‡é¢˜
                if (heading['text'] == 'æµç¨‹å›¾' and heading['type'] == 'markdown') or \
                   (heading['text'].startswith('title:') and 'mermaid' in content):
                    continue
                    
                if any(keyword in heading['text'] for keyword in priority_keywords):
                    # æ‰¾åˆ°ä¸‹ä¸€ä¸ªåŒçº§æˆ–æ›´é«˜çº§æ ‡é¢˜
                    next_heading = self._find_next_heading(heading, headings)
                    if next_heading:
                        return self._calculate_position_before_heading(content, lines, next_heading['line_num'])
                    else:
                        # å¦‚æœæ²¡æ‰¾åˆ°ä¸‹ä¸€ä¸ªæ ‡é¢˜ï¼Œåœ¨å½“å‰æ ‡é¢˜åæ’å…¥
                        return self._calculate_position_after_heading(content, lines, heading['line_num'])
        
        # ç­–ç•¥2: æŸ¥æ‰¾ç¬¬ä¸€ä¸ªäºŒçº§æ ‡é¢˜
        for heading in headings:
            if heading['level'] == 2:
                next_heading = self._find_next_heading(heading, headings)
                if next_heading:
                    return self._calculate_position_before_heading(content, lines, next_heading['line_num'])
                else:
                    return self._calculate_position_after_heading(content, lines, heading['line_num'])
        
        # ç­–ç•¥3: æŸ¥æ‰¾ç¬¬ä¸€ä¸ªä¸€çº§æ ‡é¢˜
        for heading in headings:
            if heading['level'] == 1:
                next_heading = self._find_next_heading(heading, headings)
                if next_heading:
                    return self._calculate_position_before_heading(content, lines, next_heading['line_num'])
                else:
                    return self._calculate_position_after_heading(content, lines, heading['line_num'])
        
        # ç­–ç•¥4: åœ¨ç¬¬ä¸€ä¸ªæ ‡é¢˜åæ’å…¥
        if headings:
            return self._calculate_position_after_heading(content, lines, headings[0]['line_num'])
        
        return -1

    def _find_next_heading(self, current_heading: Dict, headings: List[Dict]) -> Optional[Dict]:
        """æŸ¥æ‰¾ä¸‹ä¸€ä¸ªåŒçº§æˆ–æ›´é«˜çº§æ ‡é¢˜"""
        current_line = current_heading['line_num']
        current_level = current_heading['level']
        
        for heading in headings:
            if heading['line_num'] > current_line and heading['level'] <= current_level:
                return heading
        
        return None

    def _calculate_position_before_heading(self, content: str, lines: List[str], heading_line_num: int) -> int:
        """è®¡ç®—åœ¨æŒ‡å®šæ ‡é¢˜å‰çš„ä½ç½®"""
        target_line = '\n'.join(lines[:heading_line_num])
        return content.find(target_line) + len(target_line)

    def _calculate_position_after_heading(self, content: str, lines: List[str], heading_line_num: int) -> int:
        """è®¡ç®—åœ¨æŒ‡å®šæ ‡é¢˜åçš„ä½ç½®"""
        # æŸ¥æ‰¾æ ‡é¢˜åçš„ç¬¬ä¸€ä¸ªç©ºè¡Œ
        for i in range(heading_line_num + 1, len(lines)):
            if not lines[i].strip():
                target_line = '\n'.join(lines[:i+1])
                return content.find(target_line) + len(target_line)
        
        # å¦‚æœæ²¡æ‰¾åˆ°ç©ºè¡Œï¼Œåœ¨æ ‡é¢˜åç›´æ¥æ’å…¥
        target_line = '\n'.join(lines[:heading_line_num+1])
        return content.find(target_line) + len(target_line)


if __name__ == "__main__":
    converter = DocumentConverter()

    # æµ‹è¯•é£ä¹¦æ–‡æ¡£è½¬æ¢ï¼ˆåŒ…å«æµç¨‹å›¾ï¼‰
    feishu_url = "https://aviagames.feishu.cn/docx/My1Bdys0WobWdJxNz00cXp85nxg?from=from_copylink"
    print(f"ğŸš€ å¼€å§‹è½¬æ¢é£ä¹¦æ–‡æ¡£: {feishu_url}")
    md_file, files = converter.convert_document_unified(feishu_url)
    print(f"âœ… é£ä¹¦æ–‡æ¡£å·²è½¬æ¢ä¸º: {md_file}")
    print(f"ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶: {files}")
#
#     # æµ‹è¯•æœ¬åœ°æ–‡æ¡£è½¬æ¢ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
#     docx_file = "Bç±»æ´»åŠ¨ä¸‰åªå°çŒª-æœ¬åœ°.docx"
#     if Path(docx_file).exists():
#         print(f"\nğŸš€ å¼€å§‹è½¬æ¢æœ¬åœ°æ–‡æ¡£: {docx_file}")
#         md_file, files = converter.convert_document_unified(docx_file)
#         print(f"âœ… æœ¬åœ°æ–‡æ¡£å·²è½¬æ¢ä¸º: {md_file}")
#         print(f"ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶: {files}")
#     else:
#         print(f"â„¹ï¸ æœ¬åœ°æ–‡æ¡£ä¸å­˜åœ¨: {docx_file}")